{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f707c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Breast Cancer Prediction using KNN - Complete Pipeline\n",
    "# Wisconsin Diagnostic Breast Cancer (WDBC) Dataset\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('default')\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e8702c",
   "metadata": {},
   "source": [
    "# Step 1: Data Loading and Exploration\n",
    "\n",
    "The Wisconsin Diagnostic Breast Cancer (WDBC) dataset contains features computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. The features describe characteristics of the cell nuclei present in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f25db4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the Wisconsin Diagnostic Breast Cancer Dataset...\n",
      "Dataset Shape: (569, 31)\n",
      "Number of features: 30\n",
      "Target classes: ['malignant' 'benign']\n",
      "Target distribution:\n",
      "target\n",
      "1    357\n",
      "0    212\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class distribution:\n",
      "malignant: 212 samples (37.3%)\n",
      "benign: 357 samples (62.7%)\n"
     ]
    }
   ],
   "source": [
    "# Load the WDBC dataset\n",
    "print(\"Loading the Wisconsin Diagnostic Breast Cancer Dataset...\")\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Create a DataFrame for better handling\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target\n",
    "\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "print(f\"Number of features: {len(data.feature_names)}\")\n",
    "print(f\"Target classes: {data.target_names}\")\n",
    "print(f\"Target distribution:\")\n",
    "print(df['target'].value_counts())\n",
    "print(\"\\nClass distribution:\")\n",
    "for i, class_name in enumerate(data.target_names):\n",
    "    count = sum(df['target'] == i)\n",
    "    percentage = count / len(df) * 100\n",
    "    print(f\"{class_name}: {count} samples ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa55aae0",
   "metadata": {},
   "source": [
    "# Step 2: Data Preprocessing\n",
    "\n",
    "Since the data is already clean (no missing values) and all features are numeric, we'll prepare the features and target variables for the KNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "415ad348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing features and target variables...\n",
      "Features shape: (569, 30)\n",
      "Target shape: (569,)\n",
      "Feature names: ['mean radius', 'mean texture', 'mean perimeter', 'mean area', 'mean smoothness', 'mean compactness', 'mean concavity', 'mean concave points', 'mean symmetry', 'mean fractal dimension', 'radius error', 'texture error', 'perimeter error', 'area error', 'smoothness error', 'compactness error', 'concavity error', 'concave points error', 'symmetry error', 'fractal dimension error', 'worst radius', 'worst texture', 'worst perimeter', 'worst area', 'worst smoothness', 'worst compactness', 'worst concavity', 'worst concave points', 'worst symmetry', 'worst fractal dimension']...\n"
     ]
    }
   ],
   "source": [
    "# Prepare features and target variables\n",
    "print(\"Preparing features and target variables...\")\n",
    "\n",
    "# Features (X) and target (y)\n",
    "X = df.drop('target', axis=1)  # All features except target\n",
    "y = df['target']  # Target variable\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Feature names: {list(X.columns)}...\")  # Show all feature names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5724bd36",
   "metadata": {},
   "source": [
    "# Step 3: Train-Test Split\n",
    "\n",
    "We'll split the data before scaling to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4c5be50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data into train and test sets...\n",
      "Training set size: (455, 30)\n",
      "Testing set size: (114, 30)\n",
      "Training target distribution:\n",
      "target\n",
      "1    0.626\n",
      "0    0.374\n",
      "Name: proportion, dtype: float64\n",
      "Testing target distribution:\n",
      "target\n",
      "1    0.632\n",
      "0    0.368\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "print(\"Splitting data into train and test sets...\")\n",
    "\n",
    "# Use stratified split to maintain class proportion\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2,  # 80% for training, 20% for testing\n",
    "    random_state=42,  # For reproducibility\n",
    "    stratify=y  # Maintain class distribution\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Testing set size: {X_test.shape}\")\n",
    "print(f\"Training target distribution:\")\n",
    "print(y_train.value_counts(normalize=True).round(3))\n",
    "print(f\"Testing target distribution:\")\n",
    "print(y_test.value_counts(normalize=True).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a60554f",
   "metadata": {},
   "source": [
    "# Step 4: KNN Model Implementation\n",
    "\n",
    "Let's implement KNN with different k values to see which performs best initially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "841773d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60efeb0",
   "metadata": {},
   "source": [
    "# Step 5: Model Evaluation\n",
    "\n",
    "Now let's evaluate our best KNN model using various metrics to understand its performance comprehensively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f8cd4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
